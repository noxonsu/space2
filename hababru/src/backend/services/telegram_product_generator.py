"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ Telegram —Å–æ–æ–±—â–µ–Ω–∏–π –∫–∞–Ω–∞–ª–∞ @aideaxondemos
"""

import os
import re
import yaml
import json
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from .llm_service import LLMService


@dataclass
class TelegramMessage:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è Telegram —Å–æ–æ–±—â–µ–Ω–∏—è"""
    message_id: int
    text: str
    date: str
    media_files: List[str] = None
    
    def __post_init__(self):
        if self.media_files is None:
            self.media_files = []


class TelegramProductGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ Telegram —Å–æ–æ–±—â–µ–Ω–∏–π"""
    
    def __init__(self, llm_service: LLMService, products_dir: str = None):
        self.llm_service = llm_service
        
        if products_dir is None:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ products –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            current_dir = Path(__file__).parent
            self.products_dir = current_dir.parent.parent.parent / "content" / "products"
        else:
            self.products_dir = Path(products_dir)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.products_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
    
    def generate_product_from_message(self, message: TelegramMessage) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ Telegram —Å–æ–æ–±—â–µ–Ω–∏—è
        
        Args:
            message: –û–±—ä–µ–∫—Ç TelegramMessage —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: success, product_id, file_path –∏–ª–∏ error
        """
        try:
            self.logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥—É–∫—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è {message.message_id}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏
            duplicate_id = self._check_semantic_duplicate(message)
            if duplicate_id:
                return {
                    "success": False,
                    "error": f"–ü—Ä–æ–¥—É–∫—Ç —Å –ø–æ—Ö–æ–∂–∏–º —Å–º—ã—Å–ª–æ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {duplicate_id}",
                    "duplicate_product_id": duplicate_id,
                    "message_id": message.message_id
                }
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = self._create_product_generation_prompt(message)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            llm_response = self.llm_service.generate_text(prompt)
            
            if not llm_response:
                return {
                    "success": False,
                    "error": "LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞"
                }
            
            # –ü–∞—Ä—Å–∏–º YAML –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
            product_data = self._parse_yaml_from_llm_response(llm_response)
            
            if not product_data:
                return {
                    "success": False,
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ YAML –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"
                }
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞
            if not self._validate_product_data(product_data):
                return {
                    "success": False,
                    "error": "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é"
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞
            file_path = self._save_product_config(product_data)
            
            self.logger.info(f"–ü—Ä–æ–¥—É–∫—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {product_data['product_id']}")
            
            return {
                "success": True,
                "product_id": product_data["product_id"],
                "product_name": product_data["name"],
                "file_path": str(file_path),
                "message_id": message.message_id
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞: {str(e)}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞: {str(e)}"
            }
    
    def _create_product_generation_prompt(self, message: TelegramMessage) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ Telegram —Å–æ–æ–±—â–µ–Ω–∏—è"""
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        existing_products = self._get_existing_products()
        
        prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è B2B –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã HababRu.

–ó–ê–î–ê–ß–ê: –ù–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ Telegram –∫–∞–Ω–∞–ª–∞ —Å–æ–∑–¥–∞–π YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è: "{message.text}"
–î–∞—Ç–∞: {message.date}
–ú–µ–¥–∏–∞ —Ñ–∞–π–ª—ã: {', '.join(message.media_files) if message.media_files else '–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'}

–°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ü–†–û–î–£–ö–¢–´ (–∏–∑–±–µ–≥–∞–π –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è):
{', '.join(existing_products)}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–¢–†–£–ö–¢–£–†–ï YAML:

```yaml
---
# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞
product_id: "—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_id_–ª–∞—Ç–∏–Ω–∏—Ü–µ–π_–±–µ–∑_–ø—Ä–æ–±–µ–ª–æ–≤"
name: "–ß–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞"
description: "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö"
version: "1.0"
category: "–∫–∞—Ç–µ–≥–æ—Ä–∏—è" # ai, analytics, automation, legal, finance, marketing
status: "active"

# –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
demo_data:
  key_features:
    - "–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 1"
    - "–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 2"
    - "–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 3"
  
  supported_formats: # –µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ
    - "PDF"
    - "DOC"
  
  processing_time: "–≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏" # –µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ
  accuracy: "—Ç–æ—á–Ω–æ—Å—Ç—å –≤ %" # –µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ –¥–ª—è SEO
product_info:
  key_benefits:
    - "–ü–æ–ª—å–∑–∞ 1 —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ü–∏—Ñ—Ä–∞–º–∏"
    - "–ü–æ–ª—å–∑–∞ 2"
    - "–ü–æ–ª—å–∑–∞ 3"

  target_audience:
    - "–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è 1"
    - "–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è 2"
    - "–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è 3"

  use_cases:
    - "–°—Ü–µ–Ω–∞—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 1"
    - "–°—Ü–µ–Ω–∞—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 2"
    - "–°—Ü–µ–Ω–∞—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 3"

  demo_available: true
  screenshots: []
  
  pricing:
    basic: "–ë–∞–∑–æ–≤—ã–π —Ç–∞—Ä–∏—Ñ –æ–ø–∏—Å–∞–Ω–∏–µ"
    professional: "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–∞—Ä–∏—Ñ –æ–ø–∏—Å–∞–Ω–∏–µ"
    enterprise: "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ç–∞—Ä–∏—Ñ –æ–ø–∏—Å–∞–Ω–∏–µ"

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –≤–≤–æ–¥–∞/–≤—ã–≤–æ–¥–∞ (–∞–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ —Ç–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞)
interfaces:
  input:
    type: "object"
    properties:
      input_field:
        type: "string"
        description: "–û–ø–∏—Å–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        example: "–ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    required:
      - "input_field"

  output:
    type: "object"
    properties:
      output_field:
        type: "string"
        description: "–û–ø–∏—Å–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        example: "–ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"

# SEO –¥–∞–Ω–Ω—ã–µ
seo:
  keywords:
    - "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 1"
    - "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 2"
    - "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 3"
    # –º–∏–Ω–∏–º—É–º 10-15 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

  demo_content:
    demo_queries: # –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–µ–º–æ
      - "–ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ 1"
      - "–ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ 2"
    
    sample_results: # –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
      metric1: "–∑–Ω–∞—á–µ–Ω–∏–µ"
      metric2: "–∑–Ω–∞—á–µ–Ω–∏–µ"

# –î–µ–º–æ-–ø—Ä–∏–º–µ—Ä—ã
demo_examples:
  sample_data: # –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    - example: "–ø—Ä–∏–º–µ—Ä 1"
      result: "—Ä–µ–∑—É–ª—å—Ç–∞—Ç 1"
```

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ
2. –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞ (ai, analytics, automation, legal, finance, marketing)
3. –°–æ–∑–¥–∞–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π product_id (–ø—Ä–æ–≤–µ—Ä—å —á—Ç–æ –µ–≥–æ –Ω–µ—Ç –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö)
4. –ó–∞–ø–æ–ª–Ω–∏ –≤—Å–µ —Å–µ–∫—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
5. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π SEO –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
6. –ï—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å —ç–º–æ–¥–∑–∏ –∏–ª–∏ –æ—Å–æ–±–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —É—á—Ç–∏ —ç—Ç–æ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è
7. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û YAML –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤

–ù–∞—á–∏–Ω–∞–π —Å --- –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º YAML —Ñ–æ—Ä–º–∞—Ç–æ–º.
"""
        
        return prompt
    
    def _parse_yaml_from_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç YAML –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º YAML –±–ª–æ–∫ –≤ markdown
            yaml_match = re.search(r'```yaml\n(.*?)\n```', response, re.DOTALL)
            if yaml_match:
                yaml_content = yaml_match.group(1)
            else:
                # –ò—â–µ–º YAML –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å ---
                yaml_match = re.search(r'---(.*)', response, re.DOTALL)
                if yaml_match:
                    yaml_content = "---" + yaml_match.group(1)
                else:
                    # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ YAML
                    yaml_content = response.strip()
            
            # –ü–∞—Ä—Å–∏–º YAML
            product_data = yaml.safe_load(yaml_content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞—Ä—å (–≤–∞–ª–∏–¥–Ω—ã–π YAML –æ–±—ä–µ–∫—Ç)
            if not isinstance(product_data, dict):
                self.logger.error(f"YAML –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–æ–º: {type(product_data)}")
                return None
                
            return product_data
            
        except yaml.YAMLError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ YAML: {e}")
            return None
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è YAML –∏–∑ –æ—Ç–≤–µ—Ç–∞: {e}")
            return None
    
    def _validate_product_data(self, data: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞"""
        required_fields = [
            'product_id', 'name', 'description', 'category',
            'demo_data', 'product_info', 'seo'
        ]
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
            for field in required_fields:
                if field not in data:
                    self.logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
                    return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º product_id –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Ñ–æ—Ä–º–∞—Ç
            product_id = data['product_id']
            if not re.match(r'^[a-z0-9_]+$', product_id):
                self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç product_id: {product_id}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–æ–¥—É–∫—Ç —Å —Ç–∞–∫–∏–º ID –µ—â–µ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            existing_products = self._get_existing_products()
            if product_id in existing_products:
                self.logger.error(f"–ü—Ä–æ–¥—É–∫—Ç —Å ID {product_id} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if 'key_benefits' not in data.get('product_info', {}):
                self.logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç product_info.key_benefits")
                return False
            
            if 'keywords' not in data.get('seo', {}):
                self.logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç seo.keywords")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞: {e}")
            return False
    
    def _save_product_config(self, product_data: Dict[str, Any]) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ YAML —Ñ–∞–π–ª"""
        product_id = product_data['product_id']
        file_path = self.products_dir / f"{product_id}.yaml"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(product_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        self.logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {file_path}")
        return file_path
    
    def _get_existing_products(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        existing = []
        
        if not self.products_dir.exists():
            return existing
        
        for file_path in self.products_dir.glob("*.yaml"):
            product_id = file_path.stem
            existing.append(product_id)
        
        return existing
    
    def _check_semantic_duplicate(self, message: TelegramMessage) -> Optional[str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É–∂–µ –ø—Ä–æ–¥—É–∫—Ç —Å –ø–æ—Ö–æ–∂–∏–º —Å–º—ã—Å–ª–æ–º
        
        Args:
            message: Telegram —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            product_id —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –¥—É–±–ª—å, –∏–Ω–∞—á–µ None
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã
            existing_products = self._get_existing_products_with_data()
            
            if not existing_products:
                return None

            for product_id, product_data in existing_products.items():
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π YAML
                product_full_text = self._extract_all_text_from_product(product_data)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                is_duplicate = self._llm_semantic_comparison(message.text, product_full_text, product_id)
                
                if is_duplicate:
                    self.logger.info(f"LLM –æ–±–Ω–∞—Ä—É–∂–∏–ª —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—å: {product_id} –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message.message_id}")
                    return product_id
            
            return None
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π: {e}")
            return None
    
    def _get_existing_products_with_data(self) -> Dict[str, Dict[str, Any]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∏—Ö –¥–∞–Ω–Ω—ã–º–∏"""
        products = {}
        
        if not self.products_dir.exists():
            return products
        
        for file_path in self.products_dir.glob("*.yaml"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    product_data = yaml.safe_load(f)
                    if product_data and 'product_id' in product_data:
                        products[product_data['product_id']] = product_data
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞ {file_path}: {e}")
        
        return products

    def _extract_all_text_from_product(self, data: Any) -> str:
        """
        –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞ (dict, list, str).
        """
        texts = []
        if isinstance(data, dict):
            for key, value in data.items():
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–∞–≤–µ–¥–æ–º–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –ø–æ–ª—è
                if key in ['product_id', 'version', 'status', 'demo_available', 'screenshots', 'type', 'required', 'processing_time', 'accuracy']:
                    continue
                texts.append(self._extract_all_text_from_product(value))
        elif isinstance(data, list):
            for item in data:
                texts.append(self._extract_all_text_from_product(item))
        elif isinstance(data, str):
            texts.append(data)
        
        return " ".join(filter(None, texts))

    def _llm_semantic_comparison(self, text1: str, text2: str, product_id_for_logging: str) -> bool:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤.
        """
        prompt = f"""
–ó–ê–î–ê–ß–ê: –û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–º–∏.
–û—Ü–µ–Ω–∏, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ –æ–Ω–∏ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø—Ä–æ–¥—É–∫—Ç –∏–ª–∏ –∏–¥–µ—é, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞.

–¢–µ–∫—Å—Ç 1 (–∏–∑ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram):
---
{text1}
---

–¢–µ–∫—Å—Ç 2 (–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ "{product_id_for_logging}"):
---
{text2}
---

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ–±–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–∞–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

- –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø—Ä–æ–¥—É–∫—Ç –∏–ª–∏ –∏–¥–µ—é, –≤–µ—Ä–Ω–∏:
  {{"is_duplicate": true, "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç"}}

- –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, –≤–µ—Ä–Ω–∏:
  {{"is_duplicate": false, "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É —ç—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç"}}
"""
        try:
            response_text = self.llm_service.generate_text(prompt)
            if not response_text:
                self.logger.warning("LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
                return False

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if not json_match:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ LLM –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {response_text}")
                # –í —Å–ª—É—á–∞–µ –Ω–µ—è—Å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                return False
            
            result = json.loads(json_match.group(0))
            is_duplicate = result.get("is_duplicate", False)
            
            self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–ª—è '{product_id_for_logging}': is_duplicate={is_duplicate}, –ø—Ä–∏—á–∏–Ω–∞: {result.get('reason')}")

            return is_duplicate

        except json.JSONDecodeError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM: {e}. –û—Ç–≤–µ—Ç: {response_text}")
            return False # –ù–µ –¥—É–±–ª–∏–∫–∞—Ç, –µ—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ LLM: {e}")
            return False # –ù–µ –¥—É–±–ª–∏–∫–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

    def process_batch_messages(self, messages: List[TelegramMessage]) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥—É–∫—Ç—ã
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ Telegram —Å–æ–æ–±—â–µ–Ω–∏–π
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        results = {
            "processed": 0,
            "successful": 0,
            "failed": 0,
            "products": [],
            "errors": []
        }
        
        for message in messages:
            results["processed"] += 1
            
            result = self.generate_product_from_message(message)
            
            if result["success"]:
                results["successful"] += 1
                results["products"].append({
                    "product_id": result["product_id"],
                    "product_name": result["product_name"],
                    "message_id": result["message_id"]
                })
            else:
                results["failed"] += 1
                results["errors"].append({
                    "message_id": message.message_id,
                    "error": result["error"]
                })
        
        return results
    
    def process_all_historical_messages(self, telegram_connector) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Telegram –∫–∞–Ω–∞–ª–∞
        
        Args:
            telegram_connector: –û–±—ä–µ–∫—Ç TelegramConnector –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        try:
            self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∫–∞–Ω–∞–ª–∞ (–±–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç) —á–µ—Ä–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
            import asyncio
            
            async def fetch_messages_async():
                return await telegram_connector.fetch_all_messages()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π —Ü–∏–∫–ª —Å–æ–±—ã—Ç–∏–π
            try:
                loop = asyncio.get_running_loop()
                # –ï—Å–ª–∏ —Ü–∏–∫–ª —É–∂–µ –∑–∞–ø—É—â–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, fetch_messages_async())
                    all_messages = future.result()
            except RuntimeError:
                # –ï—Å–ª–∏ —Ü–∏–∫–ª–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                all_messages = asyncio.run(fetch_messages_async())
            
            if not all_messages:
                return {
                    "processed": 0,
                    "successful": 0,
                    "failed": 0,
                    "skipped_duplicates": 0,
                    "products": [],
                    "errors": [],
                    "duplicates": []
                }
            
            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            results = {
                "processed": 0,
                "successful": 0,
                "failed": 0,
                "skipped_duplicates": 0,
                "products": [],
                "errors": [],
                "duplicates": []
            }
            
            for message in all_messages:
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                if not self._is_suitable_for_product_generation(message):
                    continue
                
                results["processed"] += 1
                
                result = self.generate_product_from_message(message)
                
                if result["success"]:
                    results["successful"] += 1
                    results["products"].append({
                        "product_id": result["product_id"],
                        "product_name": result["product_name"],
                        "message_id": result["message_id"]
                    })
                    self.logger.info(f"–°–æ–∑–¥–∞–Ω –ø—Ä–æ–¥—É–∫—Ç: {result['product_id']}")
                    
                elif "duplicate_product_id" in result:
                    results["skipped_duplicates"] += 1
                    results["duplicates"].append({
                        "message_id": result["message_id"],
                        "duplicate_of": result["duplicate_product_id"],
                        "error": result["error"]
                    })
                    self.logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª—å: —Å–æ–æ–±—â–µ–Ω–∏–µ {result['message_id']}")
                    
                else:
                    results["failed"] += 1
                    results["errors"].append({
                        "message_id": message.message_id,
                        "error": result["error"]
                    })
                    self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {message.message_id}: {result['error']}")
            
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–∑–¥–∞–Ω–æ: {results['successful']}, –ø—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–µ–π: {results['skipped_duplicates']}, –æ—à–∏–±–æ–∫: {results['failed']}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
            return {
                "processed": 0,
                "successful": 0,
                "failed": 1,
                "skipped_duplicates": 0,
                "products": [],
                "errors": [{"error": str(e)}],
                "duplicates": []
            }
    
    def _is_suitable_for_product_generation(self, message: TelegramMessage) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞
        
        Args:
            message: Telegram —Å–æ–æ–±—â–µ–Ω–∏–µ
            
        Returns:
            True –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞
        """
        if not message.text or len(message.text.strip()) < 20:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        text_lower = message.text.lower()
        excluded_patterns = [
            "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å",
            "—Ä–µ–∫–ª–∞–º–∞",
            "—Å–ø–æ–Ω—Å–æ—Ä",
            "–ø–∞—Ä—Ç–Ω–µ—Ä",
            "@",  # —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
            "http",  # —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏
            "üì¢",  # –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            "üéâ",  # –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
        ]
        
        for pattern in excluded_patterns:
            if pattern in text_lower:
                return False
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç/—Å–µ—Ä–≤–∏—Å
        product_indicators = [
            "—Å–µ—Ä–≤–∏—Å",
            "–ø—Ä–æ–¥—É–∫—Ç", 
            "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
            "—Å–∏—Å—Ç–µ–º–∞",
            "—Ä–µ—à–µ–Ω–∏–µ",
            "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
            "–ø–æ–º–æ—â–Ω–∏–∫",
            "–∞–Ω–∞–ª–∏–∑",
            "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è",
            "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
            "–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä",
            "–±–æ—Ç",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"
        ]
        
        for indicator in product_indicators:
            if indicator in text_lower:
                return True
        
        return False
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        import re
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()
        
        return text
    
    def _extract_all_text_from_product(self, product_data: Dict[str, Any]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ YAML-–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞
        
        Args:
            product_data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∞
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π –ø—Ä–æ–¥—É–∫—Ç–∞
        """
        def extract_text_recursive(obj, texts):
            """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key not in ['product_id', 'version', 'status']:  # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
                        extract_text_recursive(value, texts)
            elif isinstance(obj, list):
                for item in obj:
                    extract_text_recursive(item, texts)
            elif isinstance(obj, str) and obj.strip():
                texts.append(obj.strip())
        
        all_texts = []
        extract_text_recursive(product_data, all_texts)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        combined_text = " | ".join(all_texts)
        
        return combined_text
    
    def _llm_semantic_comparison(self, new_text: str, existing_text: str, product_id: str) -> bool:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            new_text: –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç –∏–∑ Telegram —Å–æ–æ–±—â–µ–Ω–∏—è
            existing_text: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞
            product_id: ID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏, False –∏–Ω–∞—á–µ
        """
        try:
            prompt = f"""
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∏–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π –ø—Ä–æ–¥—É–∫—Ç/—Å–µ—Ä–≤–∏—Å.

–ù–û–í–´–ô –¢–ï–ö–°–¢ (–∏–∑ Telegram):
"{new_text}"

–°–£–©–ï–°–¢–í–£–Æ–©–ò–ô –ü–†–û–î–£–ö–¢ (ID: {product_id}):
"{existing_text}"

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–∞:
1. –û–±–∞ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø—Ä–æ–¥—É–∫—Ç—ã –æ–¥–Ω–æ–π —Å—Ñ–µ—Ä—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
3. –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è –ø–æ—Ö–æ–∂–∞
4. –†–µ—à–∞–µ–º—ã–µ –∑–∞–¥–∞—á–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã

–í–ê–ñ–ù–û: –ü—Ä–æ–¥—É–∫—Ç—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –¥—É–±–ª—è–º–∏, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ—à–∞—é—Ç –û–î–ò–ù–ê–ö–û–í–´–ï –∑–∞–¥–∞—á–∏ –≤ –û–î–ù–û–ô —Å—Ñ–µ—Ä–µ.
–ù–∞–ø—Ä–∏–º–µ—Ä:
- "–ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤" –∏ "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤" = –î–£–ë–õ–¨
- "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π" –∏ "–ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞" = –î–£–ë–õ–¨  
- "CRM –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è" –∏ "–ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤" = –ù–ï –î–£–ë–õ–¨ (—Ä–∞–∑–Ω—ã–µ —Å—Ñ–µ—Ä—ã)

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ "–î–ê" –µ—Å–ª–∏ —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç, –∏–ª–∏ "–ù–ï–¢" –µ—Å–ª–∏ —ç—Ç–æ —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.
"""
            
            response = self.llm_service.generate_text(prompt)
            
            if not response:
                self.logger.warning(f"LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å {product_id}")
                return False
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–≤–µ—Ç
            response_normalized = response.strip().upper()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
            is_duplicate = "–î–ê" in response_normalized or "YES" in response_normalized
            
            self.logger.debug(f"LLM —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {product_id}: '{response.strip()}' -> {is_duplicate}")
            
            return is_duplicate
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ LLM —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å {product_id}: {e}")
            return False
